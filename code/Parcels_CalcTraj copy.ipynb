{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0559c420-f159-40a4-b07f-d837c86d486f",
   "metadata": {},
   "source": [
    "# Calculating Lagrangian trajectories from ocean-only (uncoupled)  and ocean-wave (coupled) model simulations for the MedSea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b2ebe",
   "metadata": {},
   "source": [
    "## 1. General Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e221b09-b2b0-4698-a09d-36510bcef242",
   "metadata": {},
   "source": [
    "### 1.1 Load modules and check versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d43e785-e83e-46b1-bbd0-86c6c4cd5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from datetime import timedelta, datetime\n",
    "import math\n",
    "\n",
    "from parcels import (FieldSet, ParticleSet,\n",
    "                     JITParticle, ScipyParticle,\n",
    "                     AdvectionRK4,\n",
    "                     Variable, Field, VectorField)\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed2a6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xarray==2022.12.0\n",
      "parcels==2.4.0\n",
      "numpy==1.24.1\n"
     ]
    }
   ],
   "source": [
    "# modified after: https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook\n",
    "## make sure to use Parcels version 2.4.0 or later for zarr output\n",
    "\n",
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]            \n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "# get the version of the root package from only the name of the package\n",
    "# via cross-checking the names of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd36c2",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Define general custom functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f170d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crange(1, 1.3, 0.1) >>> [1.  1.1 1.2 1.3]\n",
      "orange(1, 1.3, 0.1) >>> [1.  1.1 1.2]\n",
      "crange(0.0, 0.6, 0.2) >>> [0.  0.2 0.4 0.6]\n",
      "orange(0.0, 0.6, 0.2) >>> [0.  0.2 0.4]\n"
     ]
    }
   ],
   "source": [
    "# DEFINE ALTERNATIVE TO USE NP.ARANGE\n",
    "#\n",
    "# from https://stackoverflow.com/questions/50299172/python-range-or-numpy-arange-with-end-limit-include\n",
    "# (code posted by Markus Dutschke on Aug 2nd 2019, last accessed on Dec 6th 2023)\n",
    "#\n",
    "# WHY using it? \n",
    "# In principal, values are generated within the half-open interval [start, stop)\n",
    "# (in other words, the interval including start but excluding stop). \n",
    "# Yet, unfortunately, when using a non-integer step (such as 0.1), \n",
    "# the results of np.arange are often not consistent. \n",
    "# Specifically, they sometimes do include the stop value.\n",
    "# It is better to use the following function based on numpy.linspace for these cases\n",
    "\n",
    "def cust_range(*args, rtol=1e-05, atol=1e-08, include=[True, False]):\n",
    "    \"\"\"\n",
    "    Combines numpy.arange and numpy.isclose to mimic\n",
    "    open, half-open and closed intervals.\n",
    "    Avoids also floating point rounding errors as with\n",
    "    >>> numpy.arange(1, 1.3, 0.1)\n",
    "    array([1. , 1.1, 1.2, 1.3])\n",
    "\n",
    "    args: [start, ]stop, [step, ]\n",
    "        as in numpy.arange\n",
    "    rtol, atol: floats\n",
    "        floating point tolerance as in numpy.isclose\n",
    "    include: boolean list-like, length 2\n",
    "        if start and end point are included\n",
    "    \"\"\"\n",
    "    # process arguments\n",
    "    if len(args) == 1:\n",
    "        start = 0\n",
    "        stop = args[0]\n",
    "        step = 1\n",
    "    elif len(args) == 2:\n",
    "        start, stop = args\n",
    "        step = 1\n",
    "    else:\n",
    "        assert len(args) == 3\n",
    "        start, stop, step = tuple(args)\n",
    "\n",
    "    # determine number of segments\n",
    "    n = (stop-start)/step + 1\n",
    "\n",
    "    # do rounding for n\n",
    "    if np.isclose(n, np.round(n), rtol=rtol, atol=atol):\n",
    "        n = np.round(n)\n",
    "\n",
    "    # correct for start/end is exluded\n",
    "    if not include[0]:\n",
    "        n -= 1\n",
    "        start += step\n",
    "    if not include[1]:\n",
    "        n -= 1\n",
    "        stop -= step\n",
    "\n",
    "    return np.linspace(start, stop, int(n))\n",
    "\n",
    "def crange(*args, **kwargs):\n",
    "    return cust_range(*args, **kwargs, include=[True, True])\n",
    "\n",
    "def orange(*args, **kwargs):\n",
    "    return cust_range(*args, **kwargs, include=[True, False])\n",
    "\n",
    "print(\"crange(1, 1.3, 0.1) >>>\", crange(1, 1.3, 0.1))\n",
    "print(\"orange(1, 1.3, 0.1) >>>\", orange(1, 1.3, 0.1))\n",
    "print(\"crange(0.0, 0.6, 0.2) >>>\", crange(0.0, 0.6, 0.2))\n",
    "print(\"orange(0.0, 0.6, 0.2) >>>\", orange(0.0, 0.6, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc2f93",
   "metadata": {},
   "source": [
    "## 2. Custom settings Parcels\n",
    "!!! IMPORTANT: need to be checked/adjusted before every execution !!! Particularly check the parts highlighted within lines of #### !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22928e",
   "metadata": {},
   "source": [
    "### 2.1 Define input paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "140b3713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate trajectories for uncoupled simulation without additional consideration of Stokes drift\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "\n",
    "# set key to 'uncoupled', 'uncoupled_sd', 'coupled_nosd', or 'coupled_sd'\n",
    "key_input = 'uncoupled'\n",
    "\n",
    "# define whether ocean velocity is sampled (key_sample_velocity = True)\n",
    "key_sample_velocity = True\n",
    "\n",
    "############################\n",
    "\n",
    "datapath = '/Users/siren/Data/IMMERSE/'\n",
    "# path were original velocity putput is stored\n",
    "\n",
    "gridpath = (datapath + 'domain/fgrid.nc')\n",
    "# path were model grid info is stored\n",
    "\n",
    "if (key_input == 'uncoupled') or (key_input == 'uncoupled_sd'):\n",
    "    filepaths_u = sorted(glob(datapath + 'surface_TKE_UNC/MED24_OBC_1d*_grid_U.nc'))\n",
    "    filepaths_v = sorted(glob(datapath + 'surface_TKE_UNC/MED24_OBC_1d*_grid_V.nc'))\n",
    "    if key_input == 'uncoupled':\n",
    "        key_stokes = False\n",
    "        print('calculate trajectories for uncoupled simulation without additional consideration of Stokes drift')\n",
    "    elif key_input == 'uncoupled_sd':\n",
    "        key_stokes = True\n",
    "        filepaths_usd = sorted(glob(datapath + 'surface_TKE_CO/MED24_OBC_1d*_grid_U.nc'))\n",
    "        filepaths_vsd = sorted(glob(datapath + 'surface_TKE_CO/MED24_OBC_1d*_grid_V.nc'))\n",
    "        print('calculate trajectories for uncoupled simulation without additional consideration of Stokes drift')\n",
    "\n",
    "elif (key_input == 'coupled_nosd') or (key_input == 'coupled_sd'):\n",
    "    filepaths_u = sorted(glob(datapath + 'surface_TKE_CO/MED24_OBC_1d*_grid_U.nc'))\n",
    "    filepaths_v = sorted(glob(datapath + 'surface_TKE_CO/MED24_OBC_1d*_grid_V.nc'))\n",
    "    if key_input == 'coupled_nosd':\n",
    "        key_stokes = False\n",
    "        print('calculate trajectories for coupled simulation without explicit consideration of Stokes drift')\n",
    "    elif key_input == 'coupled_sd':\n",
    "        key_stokes = True\n",
    "        filepaths_usd = filepaths_u\n",
    "        filepaths_vsd = filepaths_v\n",
    "        print('calculate trajectories for coupled simulation with explicit consideration of Stokes drift')\n",
    "\n",
    "else:\n",
    "    print('no (valid) key_input defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ea5f3",
   "metadata": {},
   "source": [
    "###  2.2 Define Particle release grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcb5f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum number of release positions (including land): 1537\n",
      "maximum number of particles (including land): 112201\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "\n",
    "# release_name can be \n",
    "# - \"GulfOfLion\" (particles released every 5 days over one year), or\n",
    "# - \"Test\" (particles released over one time step only)\n",
    "\n",
    "release_name = \"GulfOfLion\"\n",
    "\n",
    "############################\n",
    "\n",
    "tstart_release = 0 # first release time in time steps of input data set after start of input data set (here: daily)\n",
    "deltat_release = 5 # release from the same set of locations every deltat_release time steps\n",
    "\n",
    "times_convert = 86400 # time step of input data in seconds (here: daily)\n",
    "\n",
    "if release_name == \"Test\":\n",
    "    tend_release = 10 \n",
    "else:\n",
    "    tend_release = 365 \n",
    "# in time steps of input data set after start of input data set\n",
    "\n",
    "if (release_name == 'GulfOfLion') or (release_name == 'Test'):\n",
    "    lonmin = 3\n",
    "    lonmax = 5.6\n",
    "    latmin = 42.1\n",
    "    latmax = 43.5\n",
    "    step = 0.05 #(appr. every 5th grid cell)\n",
    "\n",
    "if release_name == 'MedSeaMid':\n",
    "    lonmin = -0.6\n",
    "    lonmax = 22\n",
    "    latmin = 31\n",
    "    latmax = 46\n",
    "    step = 0.2 #(appr. every 5th grid cell)\n",
    "\n",
    "if release_name == 'MedSeaEast':\n",
    "    lonmin = 22.2\n",
    "    lonmax = 36.2\n",
    "    latmin = 31\n",
    "    latmax = 46\n",
    "    step = 0.2 #(appr. every 5th grid cell)\n",
    "    \n",
    "if release_name == 'MedSeaWest':\n",
    "    lonmin = -18.0\n",
    "    lonmax = -0.8\n",
    "    latmin = 31\n",
    "    latmax = 46\n",
    "    step = 0.2 #(appr. every 5th grid cell)\n",
    "    \n",
    "lons, lats = np.meshgrid(crange(lonmin, lonmax, step),\n",
    "                         crange(latmin, latmax, step))\n",
    "n_releasepos = lons.shape[0]*lons.shape[1]\n",
    "n_particles = n_releasepos * ((tend_release-tstart_release)/deltat_release)\n",
    "print(\"maximum number of release positions (including land):\", n_releasepos)\n",
    "print(\"maximum number of particles (including land):\", int(n_particles))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e04e5d",
   "metadata": {},
   "source": [
    "### 2.3 Define Lagrangian trajectory integration parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1d359d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tint = 30 # in days\n",
    "deltat = 25 # in minutes\n",
    "# How to chose deltat (Schmidt et al., 2021):\n",
    "### The time step dt should be small enough that particles do not skip grid cells\n",
    "### and thereby miss characteristic oceanographic features. \n",
    "### Due to trade-off between accuracy of time stepping and computation time, the integration\n",
    "### time step dt was chosen such that particles stay in one model grid cell for at least 2dt\n",
    "### Therefore, a time step dt can be calculated following dt<dsmin/(vmax⋅2)\n",
    "### where dsmin is the smallest edge of all grid cells in the domain and\n",
    "### vmax is the largest horizontal velocity.\n",
    "### assume vmax = 1m/s, dsmin = 1/24 = 3000 m : dt < (3000 / 2)s = 1500 s = 25 min \n",
    "### (at 45N - northern boundary MedSea - 1deg lon is appr. 79km -> 1/24 deg = 3300m;\n",
    "### 1 deg lat is 111km)\n",
    "\n",
    "pclass = JITParticle # can be JITParticle or ScipyParticle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48603bb",
   "metadata": {},
   "source": [
    "### 2.4 Define Lagrangian trajectory output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84fb2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "\n",
    "outdt = 1 #in days\n",
    "\n",
    "############################\n",
    "\n",
    "procpath = '/Users/siren/IMMERSE_waves/develop-mac/data/Trajectories-revisedNEMO/' + release_name +'/'\n",
    "! mkdir -p $procpath #creates output folder if it does not exist yet\n",
    "\n",
    "if key_input == 'uncoupled':\n",
    "    outpath = (procpath + 'Parcels_CalcTraj_Data-uncoupled_Release-' + release_name)\n",
    "if key_input == 'uncoupled_sd':\n",
    "    outpath = (procpath + 'Parcels_CalcTraj_Data-uncoupled-sd_Release-' + release_name)\n",
    "elif key_input == 'coupled_nosd':\n",
    "    outpath = (procpath + 'Parcels_CalcTraj_Data-coupled-nosd_Release-' + release_name)\n",
    "elif key_input == 'coupled_sd':\n",
    "    outpath = (procpath + 'Parcels_CalcTraj_Data-coupled-sd_Release-' + release_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdafd8-a5d6-4e11-8847-519635d388d0",
   "metadata": {},
   "source": [
    "## 3. Execution of parcels simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73acd7e",
   "metadata": {},
   "source": [
    "### 3.1 Create custom Parcels functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97d87467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fieldset(keystokes=False):\n",
    "    \n",
    "    filenames = {'U': {'lon': gridpath, 'lat': gridpath, 'data': filepaths_u},\n",
    "                 'V': {'lon': gridpath, 'lat': gridpath, 'data': filepaths_v}}\n",
    "    variables = {'U': 'vozocrtx',\n",
    "                 'V': 'vomecrty'}\n",
    "    dimensions = {'U': {'lon': 'glamf', 'lat': 'gphif', 'time': 'time_counter'},\n",
    "                  'V': {'lon': 'glamf', 'lat': 'gphif', 'time': 'time_counter'}}\n",
    "    fieldset = FieldSet.from_nemo(filenames, variables, dimensions,\n",
    "                                  mesh='spherical',\n",
    "                                  allow_time_extrapolation=False,\n",
    "                                  time_periodic=False)\n",
    "        \n",
    "    if keystokes == True:\n",
    "        filenames_sd = {'U': {'lon': gridpath, 'lat': gridpath, 'data': filepaths_usd},\n",
    "                        'V': {'lon': gridpath, 'lat': gridpath, 'data': filepaths_vsd}}\n",
    "        variables_sd = {'U': 'usd',\n",
    "                        'V': 'vsd'}\n",
    "        fieldset_sd = FieldSet.from_nemo(filenames_sd, variables_sd, dimensions,\n",
    "                                         mesh='spherical',\n",
    "                                         allow_time_extrapolation=False,\n",
    "                                         time_periodic=False)\n",
    "        fieldset_sum = FieldSet(U=fieldset.U+fieldset_sd.U,\n",
    "                                V=fieldset.V+fieldset_sd.V)\n",
    "        fieldset_use = fieldset_sum\n",
    "    elif keystokes == False:\n",
    "        fieldset_use = fieldset\n",
    "        \n",
    "    return fieldset_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09ac06",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Kernels partially adopted and modified from https://github.com/OceanParcels/Parcelsv2.0PaperNorthSeaScripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "534d6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeachParticle(pclass):\n",
    "    # beached : 0 sea, 1 beached, 2 after non-beach dyn\n",
    "    beached = Variable('beached', dtype=np.int64, initial=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d126b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeachCurrentParticle(pclass):\n",
    "    # beached : 0 sea, 1 beached, 2 after non-beach dyn\n",
    "    beached = Variable('beached', dtype=np.int64, initial=0)\n",
    "    U = Variable('U', dtype=np.float32, initial=0)\n",
    "    V = Variable('V', dtype=np.float32, initial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb4a09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvectionRK4(particle, fieldset, time):\n",
    "    if particle.beached == 0:\n",
    "        (u1, v1) = fieldset.UV[time, particle.depth, particle.lat, particle.lon]\n",
    "        lon1, lat1 = (particle.lon + u1*.5*particle.dt, particle.lat + v1*.5*particle.dt)\n",
    "\n",
    "        (u2, v2) = fieldset.UV[time + .5 * particle.dt, particle.depth, lat1, lon1]\n",
    "        lon2, lat2 = (particle.lon + u2*.5*particle.dt, particle.lat + v2*.5*particle.dt)\n",
    "\n",
    "        (u3, v3) = fieldset.UV[time + .5 * particle.dt, particle.depth, lat2, lon2]\n",
    "        lon3, lat3 = (particle.lon + u3*particle.dt, particle.lat + v3*particle.dt)\n",
    "\n",
    "        (u4, v4) = fieldset.UV[time + particle.dt, particle.depth, lat3, lon3]\n",
    "        particle.lon += (u1 + 2*u2 + 2*u3 + u4) / 6. * particle.dt\n",
    "        particle.lat += (v1 + 2*v2 + 2*v3 + v4) / 6. * particle.dt\n",
    "        particle.beached = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "517e0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SampleUV(particle, fieldset, time):\n",
    "    # attention: samples particle velocity in deg/s and needs to be converted to m/s\n",
    "    # also note that fieldset is sampled at time+particle.dt, as first advection id performed and positions are updated,\n",
    "    # then fieldset is sampled at new location and time+dt, the time is updated\n",
    "    ## this procedure will probably not be necessary with future parcel releases\n",
    "    particle.U, particle.V = fieldset.UV[particle.time+particle.dt, particle.depth, particle.lat, particle.lon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1380371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BeachTesting(particle, fieldset, time):\n",
    "    if particle.beached == 2:\n",
    "        (u, v) = fieldset.UV[particle.time+particle.dt, particle.depth, particle.lat, particle.lon]\n",
    "        # if velocity threshold ist set to 1e-8 (suggestion Christian) artificial beaching occurs -> do NOT do that\n",
    "        if math.fabs(u) < 1e-14 and math.fabs(v) < 1e-14:\n",
    "            particle.beached = 1\n",
    "            print(\"Particle [%d] beached !! (%g %g %g %g)\" % (particle.id, particle.lon, particle.lat, particle.depth, particle.time+particle.dt))\n",
    "        else:\n",
    "            particle.beached = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "706903c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BeachTesting_initial(particle, fieldset, time):\n",
    "    if particle.beached == 2:\n",
    "        (u, v) = fieldset.UV[particle.time, particle.depth, particle.lat, particle.lon]\n",
    "        # if velocity threshold ist set to 1e-8 (suggestion Christian) artificial beaching occurs -> do NOT do that\n",
    "        if math.fabs(u) < 1e-14 and math.fabs(v) < 1e-14:\n",
    "            particle.beached = 1\n",
    "            print(\"Particle [%d] beached !! (%g %g %g %g)\" % (particle.id, particle.lon, particle.lat, particle.depth, particle.time))\n",
    "        else:\n",
    "            particle.beached = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "190349e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeleteParticle(particle, fieldset, time):\n",
    "    if particle.beached == 1:\n",
    "        particle.delete()\n",
    "        print(\"Particle [%d] deleted !! (%g %g %g %g)\" % (particle.id, particle.lon, particle.lat, particle.depth, particle.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6ca85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotParticleSet(pset,fieldset):\n",
    "    plotstep=4\n",
    "    pset.show(field=fieldset.U,\n",
    "              domain={'N':(latmax+plotstep/2),'S':(latmin-plotstep),\n",
    "                      'E': (lonmax+plotstep), 'W':(lonmin-plotstep)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc986d",
   "metadata": {},
   "source": [
    "### 3.2 Determine exact particle release positions (mask land positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ceb90b9-69a7-4f6f-a313-a4333df6ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release particles on predefined uniform grid, including land grid points\n",
    "fieldset_init = create_fieldset(keystokes=False)\n",
    "pset_init = ParticleSet(fieldset=fieldset_init,\n",
    "                        pclass=BeachParticle,\n",
    "                        lon=lons, lat=lats, time=0)\n",
    "pset_init.populate_indices()\n",
    "PlotParticleSet(pset_init,fieldset_init)\n",
    "print('maximum number of release positions (including land):',pset_init.lon.size)\n",
    "print()\n",
    "lons_prelim = pset_init.lon\n",
    "lats_prelim = pset_init.lat\n",
    "\n",
    "# perform a \"fake\" integration to determine and delete particles released on land\n",
    "## by making use of beach-testing and deleting kernels\n",
    "## (without advection kernel)\n",
    "kernels_init = pset_init.Kernel(BeachTesting_initial) + pset_init.Kernel(DeleteParticle)\n",
    "pset_init.execute(kernels_init, dt=0)\n",
    "PlotParticleSet(pset_init,fieldset_init)\n",
    "\n",
    "# retrieve lon and lats from all ocean release positions\n",
    "print(\"actual number of release positions (excluding land):\", pset_init.lon.size)\n",
    "print(\"actual number of particles:\",\n",
    "      pset_init.lon.size * ((tend_release-tstart_release)/deltat_release))\n",
    "lons_use = pset_init.lon\n",
    "lats_use = pset_init.lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed89d47",
   "metadata": {},
   "source": [
    "### 3.3 Perform trajectory integration and save trajectories in .zarr format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cfbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for times in range(tstart_release, tend_release, deltat_release):\n",
    "    \n",
    "    if (key_sample_velocity == True):\n",
    "        pclass_main = BeachCurrentParticle\n",
    "        outpath_ending = \"m_Sample-UV.zarr\"\n",
    "    else:\n",
    "        pclass_main=BeachParticle\n",
    "        outpath_ending = \"m.zarr\"\n",
    "  \n",
    "    ## create new clean particle set (including new clean fieldset)\n",
    "    ### ensure that for each release date the particle numbering starts again from 0\n",
    "    fieldset_main = create_fieldset(keystokes=key_stokes)\n",
    "    BeachCurrentParticle.setLastID(0)\n",
    "    BeachParticle.setLastID(0)\n",
    "    pset_main = ParticleSet(fieldset=fieldset_main,\n",
    "                            pclass=pclass_main,\n",
    "                            lon=lons_use, lat=lats_use,\n",
    "                            time=times*times_convert)   \n",
    "    pset_main.populate_indices()\n",
    "    \n",
    "    ## define output\n",
    "    ### ensure depth is not stored in trajectory output (depth not necessary for 2D calculations)\n",
    "    pset_main.set_variable_write_status(\"depth\", False) \n",
    "    outpath_final = (outpath + \"-t\" + str(times) \n",
    "                     + \"_Integrate-fw-d\" + str(tint) + \"-dt\" + str(deltat) \n",
    "                     + outpath_ending)\n",
    "    outfile_final = pset_main.ParticleFile(outpath_final, \n",
    "                                           outputdt=timedelta(days=outdt))\n",
    "  \n",
    "    ## advect particle\n",
    "    ## do not delete potentially beached particles, but keep them for statistics\n",
    "    if (key_sample_velocity == True):\n",
    "        kernels_main = (pset_main.Kernel(AdvectionRK4) + pset_main.Kernel(BeachTesting) \n",
    "                        + pset_main.Kernel(SampleUV))\n",
    "    else:\n",
    "        kernels_main = (pset_main.Kernel(AdvectionRK4) + pset_main.Kernel(BeachTesting))\n",
    "    pset_main.execute(kernels_main,\n",
    "                      runtime=timedelta(days=tint),\n",
    "                      dt=timedelta(minutes=deltat),\n",
    "                      output_file=outfile_final)\n",
    "    \n",
    "    pset_main.set_variable_write_status(\"depth\", True) # somehow needed, otherwise next pset can not be build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of final positions \n",
    "## in this simple form only works for uncoupled simulation\n",
    "## as Parcels plotting routine can not deal with summed fieldsets\n",
    "PlotParticleSet(pset_main,fieldset_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d2602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
